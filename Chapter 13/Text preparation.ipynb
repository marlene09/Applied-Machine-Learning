{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81d35737",
   "metadata": {},
   "source": [
    "Converting words and sentences into dense vector representations that incorporate meaning - using transformers\n",
    "GPT (Generative Pretrained Transformer) is literally based on the Transformer architecture.\n",
    "\n",
    "GPT stands for Generative Pretrained Transformer.\n",
    "\n",
    "The Transformer model was first introduced in the 2017 paper “Attention Is All You Need” by Vaswani et al.\n",
    "\n",
    "GPT uses only the *decoder part* of the original Transformer design (while models like BERT use the encoder part).\n",
    "\n",
    "It’s called “pretrained” because it’s first trained on massive amounts of text in a self-supervised way (predicting the next word), and then fine-tuned for specific tasks.\n",
    "\n",
    "It’s “generative” because it can generate coherent text — not just classify or encode it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041a592a",
   "metadata": {},
   "source": [
    "All NN that process text have:\n",
    "embedding layer : which uses word embeddings to transform arrays or sequences of scalar values representing words into arrays of floating point numbers called word vectors \n",
    "these vectors encode info about meanings of words and relationship between them \n",
    "ouput from an embedding layer --> input to classification layer  or other\n",
    "                              --> tranformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b5cb9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "lines = [\n",
    "    'The quick brown fox',\n",
    "    'Jumps over $$$ lazy brown dog.',\n",
    "     'Who jumps high into the blue sky after counting 123',\n",
    "     'And quickly returns to earth'\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "#Creates a dict containing all words in the input text\"\n",
    "#Creates a dict containing all words in the input text\"\n",
    "#tokenizer.fit_on_texts(lines) builds a vocabulary from all words in your input texts.\n",
    "#It assigns an integer index to each word.\n",
    "#By default, the most frequent word gets index 1, next most frequent index 2, etc.\n",
    "#Index 0 is reserved (usually for padding).\n",
    "tokenizer.fit_on_texts(lines)\n",
    "\n",
    "#Converts each sentence into a sequence of numbers using the vocabulary index.\n",
    "sequences = tokenizer.texts_to_sequences(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9d01a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the quick brown fox',\n",
       " 'jumps over lazy brown dog',\n",
       " 'who jumps over the dog high into the blue sky after counting 123',\n",
       " 'and quickly returns to earth']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences_to_text = tokenizer.sequences_to_texts(sequences)\n",
    "sequences_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf03dba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "    return ' '.join(text)\n",
    "\n",
    "lines = list(map(remove_stopwords, lines))\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "\n",
    "print(sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5302e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 6, 2, 7],\n",
       " [3, 4, 8, 2, 5],\n",
       " [9, 3, 4, 1, 5, 10, 11, 1, 12, 13, 14, 15, 16],\n",
       " [17, 18, 19, 20, 21]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = [\n",
    "    'The quick brown fox',\n",
    "     'Jumps over $$$ lazy brown dog.',\n",
    "     'Who jumps over the dog high into the blue sky after counting 123',\n",
    "     'And quickly returns to earth'\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(lines)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "sequences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_mps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
