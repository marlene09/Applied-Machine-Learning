{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e5020e4",
   "metadata": {},
   "source": [
    "Part 1 — Foundations of NLP in Deep Learning\n",
    "1️⃣ Text → Numbers\n",
    "\n",
    "Machines can’t understand words — they understand numbers.\n",
    "\n",
    "Key steps:\n",
    "\n",
    "Tokenisation – breaking text into smaller pieces (tokens)\n",
    "e.g. \"I love cats\" → [\"I\", \"love\", \"cats\"]\n",
    "\n",
    "Vocabulary – a mapping from tokens → integer IDs\n",
    "e.g. \"I\"=1, \"love\"=2, \"cats\"=3\n",
    "\n",
    "Padding/truncation – make all sequences the same length\n",
    "\n",
    "2️⃣ Representing Words: From One-Hot to Embeddings\n",
    "\n",
    "One-hot encoding: simple but inefficient (very sparse, no relationships)\n",
    "\n",
    "\"dog\" and \"cat\" are equally distant — no notion of similarity.\n",
    "\n",
    "Word embeddings: dense, learned representations of words in a continuous vector space.\n",
    "\n",
    "Words with similar meanings have similar embeddings.\n",
    "\n",
    "Learned by models like Word2Vec, GloVe, or within deep networks.\n",
    "\n",
    "💡 Example:\n",
    "king - man + woman ≈ queen\n",
    "\n",
    "Embeddings are the foundation of how models understand meaning.\n",
    "\n",
    "3️⃣ Sequence Models: RNNs\n",
    "\n",
    "Once words are vectors, you can feed them into a model that handles sequences.\n",
    "\n",
    "RNN (Recurrent Neural Network) processes tokens one by one, keeping a hidden state that carries context forward.\n",
    "\n",
    "Good for short dependencies (“I love cats”)\n",
    "\n",
    "Struggles with long sentences due to vanishing gradients (it forgets long-term context)\n",
    "\n",
    "4️⃣ Improvement: LSTM (Long Short-Term Memory)\n",
    "\n",
    "An LSTM is a special kind of RNN designed to remember longer-term information.\n",
    "\n",
    "It uses gates:\n",
    "\n",
    "Input gate: decide what new info to store\n",
    "\n",
    "Forget gate: decide what to throw away\n",
    "\n",
    "Output gate: decide what to pass on\n",
    "\n",
    "🧠 LSTMs = “RNNs with memory and control.”\n",
    "They were the workhorse for early translation and speech models before Transformers.\n",
    "\n",
    "5️⃣ Encoder–Decoder Architecture (Seq2Seq)\n",
    "\n",
    "To handle tasks like translation, you need two parts:\n",
    "\n",
    "Encoder: reads the source sentence (e.g. English)\n",
    "\n",
    "Decoder: generates the target sentence (e.g. French)\n",
    "\n",
    "Both can be built using LSTMs.\n",
    "\n",
    "💬 Example:\n",
    "\n",
    "Input: “I love cats.”\n",
    "Output: “J’aime les chats.”\n",
    "\n",
    "But pure encoder–decoder LSTMs still struggle with long sentences, because the encoder squashes the entire meaning into a single vector.\n",
    "\n",
    "6️⃣ Attention Mechanism\n",
    "\n",
    "This was a major breakthrough before Transformers.\n",
    "\n",
    "Instead of relying on one “summary vector”, the attention mechanism lets the decoder “look back” at the encoder’s outputs directly, deciding which parts of the input sentence are most relevant at each translation step.\n",
    "\n",
    "💡 Example:\n",
    "When translating “cats”, the decoder can pay attention to the word “cats” in the input rather than relying on memory alone.\n",
    "\n",
    "🪄 7️⃣ Leading to the Transformer\n",
    "\n",
    "Transformers remove recurrence entirely and rely purely on:\n",
    "\n",
    "Self-Attention (each word looks at all other words)\n",
    "\n",
    "Positional encoding (since order matters)\n",
    "\n",
    "This gives:\n",
    "\n",
    "Parallelisation (faster training)\n",
    "\n",
    "Better long-term dependency handling\n",
    "\n",
    "State-of-the-art performance in translation and beyond.\n",
    "\n",
    "So the “bilingual Transformer-based NMT” section you’re entering builds on all of this:\n",
    "\n",
    "Concept\tRole in the journey\n",
    "Tokenisation\tTurns text into numbers\n",
    "Embeddings\tTurns numbers into meaningful vectors\n",
    "RNNs\tProcess sequences in order\n",
    "LSTMs\tImprove memory over time\n",
    "Seq2Seq\tEncode → decode for translation\n",
    "Attention\tFocus on important words\n",
    "Transformer\tPure attention → modern NMT models"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
